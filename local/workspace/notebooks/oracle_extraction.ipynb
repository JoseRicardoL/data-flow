{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AWS Glue ETL - Oracle Extraction Development Notebook\n",
    "\n",
    "Este notebook permite desarrollar y probar el ETL de Oracle Extraction en un entorno local que replica AWS Glue. Utiliza las mismas bibliotecas, frameworks y patrones que se usarían en AWS Glue en la nube."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Inicialización del Entorno AWS Glue\n",
    "\n",
    "Primero, configuramos el entorno de ejecución de AWS Glue con SparkContext, GlueContext y Job:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importaciones de AWS Glue\n",
    "import sys\n",
    "from awsglue.transforms import *\n",
    "from awsglue.utils import getResolvedOptions\n",
    "from pyspark.context import SparkContext\n",
    "from awsglue.context import GlueContext\n",
    "from awsglue.job import Job\n",
    "\n",
    "# Importaciones para el ETL\n",
    "from pyspark.sql.functions import col, when, concat_ws\n",
    "import boto3\n",
    "import json\n",
    "import logging\n",
    "from io import StringIO\n",
    "\n",
    "# Configurar logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger()\n",
    "\n",
    "# Inicializar contexto Glue y Spark\n",
    "sc = SparkContext()\n",
    "glueContext = GlueContext(sc)\n",
    "spark = glueContext.spark_session\n",
    "job = Job(glueContext)\n",
    "\n",
    "# Verificar versiones\n",
    "print(f\"Spark version: {spark.version}\")\n",
    "print(f\"Python version: {sys.version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuración de Parámetros del Job\n",
    "\n",
    "Simulamos los parámetros que normalmente se pasarían al job de Glue:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir parámetros de prueba\n",
    "test_input = [\n",
    "    {\n",
    "        \"enterprise\": \"URBANOS DE SEGOVIA\",\n",
    "        \"body\": json.dumps({\n",
    "            \"P_EMPRESA\": \"URBANOS DE SEGOVIA\",\n",
    "            \"P_CONTR\": \"001\",\n",
    "            \"P_CONTR_FLAG\": \"False\",\n",
    "            \"P_VERSION\": \"20240101_20241231\",\n",
    "            \"P_FECHAD\": \"20240101\",\n",
    "            \"P_FECHAH\": \"20241231\",\n",
    "            \"P_ENV\": \"dev\",\n",
    "            \"P_LINE\": \"001\"\n",
    "        }),\n",
    "        \"id\": \"1\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Mock de getResolvedOptions\n",
    "def mock_getResolvedOptions(argv, options):\n",
    "    return {\n",
    "        \"JOB_NAME\": \"oracle-extraction-dev\",\n",
    "        \"bronze_bucket\": \"mado-gtfs-dev-eu-west-1-992807582431-bronze\",\n",
    "        \"json_input\": json.dumps(test_input)\n",
    "    }\n",
    "\n",
    "# Reemplazar getResolvedOptions\n",
    "import awsglue.utils\n",
    "awsglue.utils.getResolvedOptions = mock_getResolvedOptions\n",
    "\n",
    "# Obtener argumentos del trabajo\n",
    "args = getResolvedOptions(sys.argv, [\"JOB_NAME\", \"bronze_bucket\", \"json_input\"])\n",
    "\n",
    "# Inicializar el job\n",
    "job.init(args[\"JOB_NAME\"], args)\n",
    "\n",
    "# Extraer parámetros\n",
    "bronze_bucket = args[\"bronze_bucket\"]\n",
    "json_input = args[\"json_input\"]\n",
    "explotations = json.loads(json_input)\n",
    "\n",
    "# Extraer parámetros específicos\n",
    "p_empresa = json.loads(explotations[0][\"body\"])[\"P_EMPRESA\"]\n",
    "p_contr = json.loads(explotations[0][\"body\"])[\"P_CONTR\"]\n",
    "p_contr_flag = json.loads(explotations[0][\"body\"])[\"P_CONTR_FLAG\"]\n",
    "p_version = json.loads(explotations[0][\"body\"])[\"P_VERSION\"]\n",
    "p_fechad = json.loads(explotations[0][\"body\"])[\"P_FECHAD\"]\n",
    "p_fechah = json.loads(explotations[0][\"body\"])[\"P_FECHAH\"]\n",
    "p_env = json.loads(explotations[0][\"body\"])[\"P_ENV\"]\n",
    "\n",
    "# Extraer líneas\n",
    "p_lines = []\n",
    "for explotation in explotations:\n",
    "    p_lines.append(json.loads(explotation[\"body\"])[\"P_LINE\"])\n",
    "lineas_activas = tuple(p_lines)\n",
    "\n",
    "# Mostrar parámetros configurados\n",
    "print(f\"Parámetros configurados:\")\n",
    "print(f\"- Empresa: {p_empresa}\")\n",
    "print(f\"- Contrato: {p_contr}\")\n",
    "print(f\"- Versión: {p_version}\")\n",
    "print(f\"- Fechas: {p_fechad} - {p_fechah}\")\n",
    "print(f\"- Líneas: {lineas_activas}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Configuración de Secretos y Conexión a Oracle\n",
    "\n",
    "Para conectar a Oracle, necesitamos obtener los secretos de AWS o configurar una conexión local:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_secret(secret_name, region_name=\"eu-west-1\", use_local=False):\n",
    "    if use_local:\n",
    "        mock_secrets = {\n",
    "            \"/gtfs/on-prem/gestra\": {\n",
    "                \"username\": \"gestra_user\",\n",
    "                \"password\": \"gestra_password\",\n",
    "                \"host\": \"oracle-host\",\n",
    "                \"port\": \"1521\",\n",
    "                \"service\": \"XEPDB1\"\n",
    "            }\n",
    "        }\n",
    "        return mock_secrets.get(secret_name)\n",
    "    else:\n",
    "        try:\n",
    "            session = boto3.session.Session()\n",
    "            client = session.client(\n",
    "                service_name='secretsmanager',\n",
    "                region_name=region_name\n",
    "            )\n",
    "            get_secret_value_response = client.get_secret_value(SecretId=secret_name)\n",
    "            \n",
    "            if 'SecretString' in get_secret_value_response:\n",
    "                secret = get_secret_value_response['SecretString']\n",
    "                return json.loads(secret)\n",
    "        except Exception as e:\n",
    "            print(f\"Error obteniendo secreto: {e}\")\n",
    "            return None\n",
    "\n",
    "try:\n",
    "    gestra_secret = get_secret(\"/gtfs/on-prem/gestra\")\n",
    "    if gestra_secret:\n",
    "        print(\"✅ Secreto obtenido correctamente de AWS Secrets Manager\")\n",
    "        # Mostrar detalles del secreto (sin la contraseña)\n",
    "        safe_secret = {k: (\"******\" if k == \"password\" else v) for k, v in gestra_secret.items()}\n",
    "        print(f\"Detalles: {safe_secret}\")\n",
    "    else:\n",
    "        print(\"⚠️ No se pudo obtener el secreto de AWS, usando configuración local\")\n",
    "        gestra_secret = get_secret(\"/gtfs/on-prem/gestra\", use_local=True)\n",
    "except Exception as e:\n",
    "    print(f\"⚠️ Error conectando a AWS: {e}\")\n",
    "    print(\"Usando configuración local para pruebas\")\n",
    "    gestra_secret = get_secret(\"/gtfs/on-prem/gestra\", use_local=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuración JDBC para Oracle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurar conexión JDBC para Oracle (si tenemos secretos)\n",
    "if gestra_secret:\n",
    "    # Formato 1 (estándar)\n",
    "    jdbc_url = f\"jdbc:oracle:thin:@//{gestra_secret['host']}:{gestra_secret['port']}/{gestra_secret['service']}\"\n",
    "    \n",
    "    # Formato 2 (alternativo si el primero no funciona)\n",
    "    # jdbc_url = f\"jdbc:oracle:thin:{gestra_secret['username']}/{gestra_secret['password']}@{gestra_secret['host']}:{gestra_secret['port']}:{gestra_secret['service']}\"\n",
    "    \n",
    "    # Formato 3 (con descripción completa)\n",
    "    # jdbc_url = f\"jdbc:oracle:thin:@(DESCRIPTION=(ADDRESS=(PROTOCOL=TCP)(HOST={gestra_secret['host']})(PORT={gestra_secret['port']}))(CONNECT_DATA=(SERVICE_NAME={gestra_secret['service']}))\"\n",
    "    \n",
    "    connection_properties = {\n",
    "        \"user\": gestra_secret['username'],\n",
    "        \"password\": gestra_secret['password'],\n",
    "        \"driver\": \"oracle.jdbc.driver.OracleDriver\"\n",
    "    }\n",
    "    \n",
    "    print(f\"JDBC URL: {jdbc_url}\")\n",
    "    print(f\"Propiedades: {connection_properties['user']}, driver: {connection_properties['driver']}\")\n",
    "else:\n",
    "    print(\"⚠️ No se ha configurado la conexión JDBC (secretos no disponibles)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Prueba de Conectividad a Oracle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probar conexión a Oracle usando una consulta simple\n",
    "if gestra_secret:\n",
    "    try:\n",
    "        # Intentar ejecutar una consulta básica\n",
    "        test_query = \"\"\"SELECT 1 as TEST_CONNECTION FROM DUAL\"\"\"\n",
    "        \n",
    "        test_df = spark.read.format(\"jdbc\").options(\n",
    "            url=jdbc_url,\n",
    "            dbtable=f\"({test_query})\",\n",
    "            user=connection_properties[\"user\"],\n",
    "            password=connection_properties[\"password\"],\n",
    "            driver=connection_properties[\"driver\"]\n",
    "        ).load()\n",
    "        \n",
    "        # Mostrar resultados\n",
    "        test_df.show()\n",
    "        print(\"✅ Conexión a Oracle exitosa\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error al conectar a Oracle: {e}\")\n",
    "        print(\"\\nEsto puede deberse a:\")\n",
    "        print(\"  - Credenciales incorrectas\")\n",
    "        print(\"  - Problemas de red/firewall\")\n",
    "        print(\"  - Formato incorrecto de URL JDBC\")\n",
    "        print(\"  - Driver Oracle no disponible\")\n",
    "else:\n",
    "    print(\"⚠️ No se puede probar la conexión sin configuración\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Crear Contract Codes\n",
    "\n",
    "Ahora creamos la tabla `contract_codes` que se usa en el ETL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear contract codes como lo hace el ETL\n",
    "contract_codes = []\n",
    "for line in p_lines:\n",
    "    contract_codes.append(\n",
    "        spark.sql(\n",
    "            f\"SELECT '{p_empresa}' AS EMPRESA, '{line}' AS LINE, '{p_contr}' AS CONTRACT_CODE\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Union all the DataFrames\n",
    "final_contract_codes_df = contract_codes[0]\n",
    "for df in contract_codes[1:]:\n",
    "    final_contract_codes_df = final_contract_codes_df.union(df)\n",
    "\n",
    "final_contract_codes_df.createOrReplaceTempView(\"contract_codes\")\n",
    "result = spark.sql(\"SELECT * FROM contract_codes\")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Consulta de Rutas (Routes)\n",
    "\n",
    "Probemos la consulta routes que utiliza el ETL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir la consulta de rutas\n",
    "routes_query = f'''\n",
    "                    select\n",
    "                        CAST(TRIM(t.CODLIN) AS STRING) as route_id,\n",
    "                        CAST('{p_empresa}' AS STRING)||'_'||CAST('{p_contr}' AS STRING) as agency_id,\n",
    "                        CAST(COALESCE(wb.CODLINWEB,t.CODLIN) AS STRING) as route_short_name,\n",
    "                        CAST(COALESCE(wb.NOMWEB,t.NOMLIN) AS STRING) as route_long_name,\n",
    "                        NULL as route_desc,\n",
    "                        '3'   as route_type,\n",
    "                        NULL as route_url,\n",
    "                        COALESCE(wb.COLOR, 'CC0000') as route_color,\n",
    "                        COALESCE(wb.COLOR, 'FFFFFF') as route_text_color,\n",
    "                        NULL as route_sort_order,\n",
    "                        NULL as continuous_pickup,\n",
    "                        NULL as continuous_drop_off\n",
    "                    from MAELIN t\n",
    "                    left join PB_LINWEB wb on (wb.EMPRESA = t.EMPRESA and wb.CODLIN = t.CODLIN) \n",
    "                    INNER JOIN ( SELECT EMPRESA, LINE, CONTRACT_CODE \n",
    "                        FROM ( contract_codes) \n",
    "                    ) lc ON t.empresa = lc.EMPRESA AND t.codlin = lc.LINE\n",
    "                    where t.EMPRESA = '{p_empresa}' and lc.CONTRACT_CODE = '{p_contr}'\n",
    "                    and t.ESTADO = 1 \n",
    "                    and t.ACTWEB = 'S'\n",
    "                    and t.ACTGTR = 'S'\n",
    "                    and t.CODLIN in (select distinct codlin from PB_CALENDCMR\n",
    "                        where empresa = '{p_empresa}' \n",
    "                        and TO_DATE(fecha, 'yyyy-MM-dd') BETWEEN TO_DATE('{p_fechad}', 'yyyyMMdd') AND TO_DATE('{p_fechah}', 'yyyyMMdd')\n",
    "                    )\n",
    "            '''\n",
    "\n",
    "print(f\"Consulta SQL generada:\")\n",
    "print(routes_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejecutar la consulta si tenemos conexión Oracle\n",
    "if gestra_secret:\n",
    "    try:\n",
    "        routes_df = spark.read.format(\"jdbc\").options(\n",
    "            url=jdbc_url,\n",
    "            dbtable=f\"({routes_query})\",\n",
    "            user=connection_properties[\"user\"],\n",
    "            password=connection_properties[\"password\"],\n",
    "            driver=connection_properties[\"driver\"]\n",
    "        ).load()\n",
    "        \n",
    "        # Mostrar esquema y datos\n",
    "        print(\"Esquema:\")\n",
    "        routes_df.printSchema()\n",
    "        \n",
    "        print(\"\\nDatos:\")\n",
    "        routes_df.show()\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error ejecutando consulta: {e}\")\n",
    "else:\n",
    "    print(\"⚠️ No se puede ejecutar la consulta sin conexión Oracle\")\n",
    "    # Crear datos de ejemplo para desarrollo\n",
    "    print(\"Creando datos de ejemplo para desarrollo...\")\n",
    "    \n",
    "    routes_test_data = [\n",
    "        (\"001\", f\"{p_empresa}_{p_contr}\", \"001\", \"Línea 1 - Centro\", None, \"3\", None, \"CC0000\", \"FFFFFF\", None, None, None),\n",
    "        (\"002\", f\"{p_empresa}_{p_contr}\", \"002\", \"Línea 2 - Estación\", None, \"3\", None, \"0000CC\", \"FFFFFF\", None, None, None),\n",
    "    ]\n",
    "    \n",
    "    routes_columns = [\"route_id\", \"agency_id\", \"route_short_name\", \"route_long_name\", \"route_desc\", \n",
    "                   \"route_type\", \"route_url\", \"route_color\", \"route_text_color\", \n",
    "                   \"route_sort_order\", \"continuous_pickup\", \"continuous_drop_off\"]\n",
    "    \n",
    "    routes_df = spark.createDataFrame(routes_test_data, routes_columns)\n",
    "    routes_df.printSchema()\n",
    "    routes_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Función Process del ETL\n",
    "\n",
    "Ahora probemos la función principal `process` del ETL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir la función process como en el ETL original\n",
    "def process(queries, P_EMPRESA, P_VERSION, P_CONTR, P_FECHAD, P_FECHAH, contract_codes):\n",
    "    \"\"\"\n",
    "    Procesa las tablas: routes, stops, trips, calendar,\n",
    "    calendar_dates, stop_times, trayectos\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if \"shapes\" in queries.items():\n",
    "            return\n",
    "        else:\n",
    "            for file_name, query in queries.items():\n",
    "                try:\n",
    "                    contract_codes_sql = spark.sql(\"SELECT * FROM contract_codes\").toPandas()\n",
    "                    sql_for_copy = \"WITH contract_codes AS (\\n\"\n",
    "                    for i, row in contract_codes_sql.iterrows():\n",
    "                        sql_for_copy += f\"    SELECT '{row['EMPRESA']}' AS EMPRESA, '{row['LINE']}' AS LINE, '{row['CONTRACT_CODE']}' AS CONTRACT_CODE FROM DUAL\"\n",
    "                        if i < len(contract_codes_sql) - 1:\n",
    "                            sql_for_copy += \" UNION ALL\\n\"\n",
    "                        else:\n",
    "                            sql_for_copy += \"\\n\"\n",
    "                    sql_for_copy += \")\\n\" + query.replace(\"( contract_codes )\", \"contract_codes\")\n",
    "                    logging.info(f\"\\n===== SQL DE ({file_name}) =====\")\n",
    "                    logging.info(sql_for_copy)\n",
    "                except Exception as e:\n",
    "                    logging.error(f\"Error al crear SQL para depuración: {str(e)}\")\n",
    "\n",
    "                file_path = \"\".join(\n",
    "                    f\"GTFS/{file_name.upper()}\"\n",
    "                    f\"/explotation={P_EMPRESA}\"\n",
    "                    f\"/contract={P_CONTR}\"\n",
    "                    f\"/version={P_VERSION}/\"\n",
    "                )\n",
    "\n",
    "                logging.info(f\"Procesando {file_name}...\")\n",
    "                \n",
    "                if gestra_secret:\n",
    "                    # Ejecutar la consulta contra Oracle\n",
    "                    try:\n",
    "                        df = spark.read.format(\"jdbc\").options(\n",
    "                            url=jdbc_url,\n",
    "                            dbtable=f\"({query})\",\n",
    "                            user=connection_properties[\"user\"],\n",
    "                            password=connection_properties[\"password\"],\n",
    "                            driver=connection_properties[\"driver\"]\n",
    "                        ).load()\n",
    "                        logging.info(f\"Consulta ejecutada correctamente para {file_name}\")\n",
    "                    except Exception as e:\n",
    "                        logging.error(f\"Error al ejecutar consulta para {file_name}: {str(e)}\")\n",
    "                        # Crear DataFrame vacío con estructura esperada para continuar\n",
    "                        if file_name == \"routes\":\n",
    "                            columns = [\"route_id\", \"agency_id\", \"route_short_name\", \"route_long_name\", \"route_desc\", \n",
    "                                      \"route_type\", \"route_url\", \"route_color\", \"route_text_color\", \n",
    "                                      \"route_sort_order\", \"continuous_pickup\", \"continuous_drop_off\"]\n",
    "                            df = spark.createDataFrame([], schema=columns)\n",
    "                        else:\n",
    "                            df = spark.createDataFrame([])\n",
    "                else:\n",
    "                    # Modo offline - crear datos de ejemplo\n",
    "                    logging.info(f\"Modo OFFLINE: Generando datos de ejemplo para {file_name}\")\n",
    "                    if file_name == \"routes\":\n",
    "                        df = routes_df  # Usar el DataFrame de ejemplo creado antes\n",
    "                    else:\n",
    "                        # Crear DataFrame vacío para otros archivos\n",
    "                        df = spark.createDataFrame([])\n",
    "                \n",
    "                # Mostrar información del DataFrame\n",
    "                logging.info(f\"Esquema para {file_name}:\")\n",
    "                df.printSchema()\n",
    "                logging.info(f\"Muestra de datos para {file_name}:\")\n",
    "                df.show(5)\n",
    "                \n",
    "                # Simular escritura de archivo\n",
    "                try:\n",
    "                    logging.info(f\"Simulando escritura en {file_path}{file_name}.txt\")\n",
    "                    # Aquí escribiríamos el archivo en S3 usando la lógica del ETL original\n",
    "                    # ...\n",
    "                except Exception as e:\n",
    "                    logging.error(f\"Error simulando escritura: {str(e)}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error processing the event: {e}\")\n",
    "        raise\n",
    "\n",
    "print(\"Función process definida correctamente\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prueba de Ejecución de la Función Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir consultas para probar\n",
    "test_queries = {\n",
    "    \"routes\": routes_query\n",
    "}\n",
    "\n",
    "# Ejecutar process\n",
    "process(test_queries, p_empresa, p_version, p_contr, p_fechad, p_fechah, final_contract_codes_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Emulación de Salida del Job\n",
    "\n",
    "Finalmente, emulamos la salida y el commit del Job como lo haría el script original:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generar salida similar a la del script original\n",
    "response = {\n",
    "    \"statusCode\": 200,\n",
    "    \"body\": json.dumps(\n",
    "        {\n",
    "            \"P_EMPRESA\": p_empresa,\n",
    "            \"P_VERSION\": p_version,\n",
    "            \"P_FECHAD\": p_fechad,\n",
    "            \"P_FECHAH\": p_fechah,\n",
    "            \"P_CONTR\": p_contr,\n",
    "        }\n",
    "    ),\n",
    "}\n",
    "\n",
    "print(\"Respuesta final del job:\")\n",
    "print(json.dumps(response, indent=2))\n",
    "\n",
    "# Commit del job\n",
    "job.commit()\n",
    "print(\"\\nJob completado exitosamente\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
